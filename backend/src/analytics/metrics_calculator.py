"""
Metrics Calculator
=================

Advanced metrics calculation engine for financial analytics.
Provides real-time and batch calculation of business metrics, KPIs, and performance indicators.
"""

import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple
from decimal import Decimal, ROUND_HALF_UP
from dataclasses import dataclass
from enum import Enum
import logging
from sqlalchemy.orm import Session
from sqlalchemy import func, and_, or_, text

from .data_models import (
    TransactionAnalytics, CustomerAnalytics, BusinessMetrics,
    PerformanceMetrics
)


class MetricType(Enum):
    """Types of metrics that can be calculated."""
    FINANCIAL = "financial"
    OPERATIONAL = "operational"
    RISK = "risk"
    CUSTOMER = "customer"
    PERFORMANCE = "performance"
    COMPLIANCE = "compliance"


class AggregationType(Enum):
    """Types of aggregation for metrics."""
    SUM = "sum"
    AVERAGE = "average"
    COUNT = "count"
    MIN = "min"
    MAX = "max"
    MEDIAN = "median"
    PERCENTILE = "percentile"
    STANDARD_DEVIATION = "std_dev"
    VARIANCE = "variance"


@dataclass
class MetricDefinition:
    """Definition of a metric calculation."""
    name: str
    type: MetricType
    description: str
    calculation_method: str
    aggregation: AggregationType
    data_source: str
    filters: Dict[str, Any] = None
    parameters: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.filters is None:
            self.filters = {}
        if self.parameters is None:
            self.parameters = {}


@dataclass
class MetricResult:
    """Result of a metric calculation."""
    metric_name: str
    value: float
    previous_value: Optional[float]
    change_percentage: Optional[float]
    calculation_timestamp: datetime
    period_start: datetime
    period_end: datetime
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


class MetricsCalculator:
    """
    Advanced metrics calculation engine for financial analytics.
    
    Features:
    - Real-time metric calculation
    - Historical trend analysis
    - Comparative analysis
    - Automated alerting
    - Performance optimization
    """
    
    def __init__(self, db_session: Session):
        self.db = db_session
        self.logger = logging.getLogger(__name__)
        self._metric_definitions = self._load_default_metrics()
        self._calculation_cache = {}
    
    def calculate_metric(self, metric_name: str, start_date: datetime, 
                        end_date: datetime, filters: Dict[str, Any] = None) -> MetricResult:
        """
        Calculate a specific metric for the given time period.
        
        Args:
            metric_name: Name of the metric to calculate
            start_date: Start of the calculation period
            end_date: End of the calculation period
            filters: Additional filters to apply
            
        Returns:
            MetricResult containing the calculated value and metadata
        """
        
        if metric_name not in self._metric_definitions:
            raise ValueError(f"Unknown metric: {metric_name}")
        
        metric_def = self._metric_definitions[metric_name]
        
        # Merge filters
        combined_filters = {**metric_def.filters}
        if filters:
            combined_filters.update(filters)
        
        try:
            # Calculate current value
            current_value = self._calculate_metric_value(
                metric_def, start_date, end_date, combined_filters
            )
            
            # Calculate previous period value for comparison
            period_duration = end_date - start_date
            prev_start = start_date - period_duration
            prev_end = start_date
            
            previous_value = self._calculate_metric_value(
                metric_def, prev_start, prev_end, combined_filters
            )
            
            # Calculate change percentage
            change_percentage = None
            if previous_value and previous_value != 0:
                change_percentage = ((current_value - previous_value) / previous_value) * 100
            
            result = MetricResult(
                metric_name=metric_name,
                value=current_value,
                previous_value=previous_value,
                change_percentage=change_percentage,
                calculation_timestamp=datetime.utcnow(),
                period_start=start_date,
                period_end=end_date,
                metadata={
                    'metric_type': metric_def.type.value,
                    'aggregation': metric_def.aggregation.value,
                    'filters_applied': combined_filters
                }
            )
            
            self.logger.info(f"Calculated metric {metric_name}: {current_value}")
            return result
            
        except Exception as e:
            self.logger.error(f"Error calculating metric {metric_name}: {str(e)}")
            raise
    
    def calculate_multiple_metrics(self, metric_names: List[str], start_date: datetime,
                                 end_date: datetime, filters: Dict[str, Any] = None) -> List[MetricResult]:
        """Calculate multiple metrics in batch."""
        
        results = []
        for metric_name in metric_names:
            try:
                result = self.calculate_metric(metric_name, start_date, end_date, filters)
                results.append(result)
            except Exception as e:
                self.logger.error(f"Error calculating metric {metric_name}: {str(e)}")
                # Continue with other metrics
        
        return results
    
    def _calculate_metric_value(self, metric_def: MetricDefinition, start_date: datetime,
                               end_date: datetime, filters: Dict[str, Any]) -> float:
        """Calculate the actual metric value based on definition."""
        
        if metric_def.data_source == "transactions":
            return self._calculate_transaction_metric(metric_def, start_date, end_date, filters)
        elif metric_def.data_source == "customers":
            return self._calculate_customer_metric(metric_def, start_date, end_date, filters)
        elif metric_def.data_source == "performance":
            return self._calculate_performance_metric(metric_def, start_date, end_date, filters)
        else:
            raise ValueError(f"Unknown data source: {metric_def.data_source}")
    
    def _calculate_transaction_metric(self, metric_def: MetricDefinition, start_date: datetime,
                                    end_date: datetime, filters: Dict[str, Any]) -> float:
        """Calculate metrics based on transaction data."""
        
        # Base query
        query = self.db.query(TransactionAnalytics).filter(
            and_(
                TransactionAnalytics.transaction_date >= start_date,
                TransactionAnalytics.transaction_date <= end_date
            )
        )
        
        # Apply filters
        if 'currency' in filters:
            query = query.filter(TransactionAnalytics.currency.in_(filters['currency']))
        if 'transaction_type' in filters:
            query = query.filter(TransactionAnalytics.transaction_type.in_(filters['transaction_type']))
        if 'country_code' in filters:
            query = query.filter(TransactionAnalytics.country_code.in_(filters['country_code']))
        if 'min_amount' in filters:
            query = query.filter(TransactionAnalytics.amount >= filters['min_amount'])
        if 'max_amount' in filters:
            query = query.filter(TransactionAnalytics.amount <= filters['max_amount'])
        
        # Calculate based on metric name and aggregation
        if metric_def.name == "total_transaction_count":
            return float(query.count())
        
        elif metric_def.name == "total_transaction_volume":
            result = query.with_entities(func.sum(TransactionAnalytics.amount)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "average_transaction_amount":
            result = query.with_entities(func.avg(TransactionAnalytics.amount)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "median_transaction_amount":
            amounts = [float(t.amount) for t in query.all()]
            return float(np.median(amounts)) if amounts else 0.0
        
        elif metric_def.name == "transaction_volume_growth_rate":
            # This requires comparison with previous period
            current_volume = query.with_entities(func.sum(TransactionAnalytics.amount)).scalar()
            return float(current_volume) if current_volume else 0.0
        
        elif metric_def.name == "high_risk_transaction_ratio":
            total_count = query.count()
            high_risk_count = query.filter(TransactionAnalytics.risk_score > 0.7).count()
            return (high_risk_count / total_count) if total_count > 0 else 0.0
        
        elif metric_def.name == "fraud_detection_rate":
            total_count = query.count()
            fraud_count = query.filter(TransactionAnalytics.fraud_probability > 0.5).count()
            return (fraud_count / total_count) if total_count > 0 else 0.0
        
        elif metric_def.name == "average_risk_score":
            result = query.with_entities(func.avg(TransactionAnalytics.risk_score)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "cross_border_transaction_ratio":
            total_count = query.count()
            # Assuming domestic transactions have specific country codes
            cross_border_count = query.filter(
                TransactionAnalytics.country_code.notin_(['US', 'domestic'])
            ).count()
            return (cross_border_count / total_count) if total_count > 0 else 0.0
        
        elif metric_def.name == "large_transaction_count":
            # Transactions over $10,000
            return float(query.filter(TransactionAnalytics.amount > 10000).count())
        
        elif metric_def.name == "suspicious_activity_ratio":
            total_count = query.count()
            suspicious_count = query.filter(TransactionAnalytics.suspicious_activity == True).count()
            return (suspicious_count / total_count) if total_count > 0 else 0.0
        
        elif metric_def.name == "revenue_estimate":
            # Assuming 2.9% transaction fee
            total_volume = query.with_entities(func.sum(TransactionAnalytics.amount)).scalar()
            return float(total_volume * Decimal('0.029')) if total_volume else 0.0
        
        else:
            raise ValueError(f"Unknown transaction metric: {metric_def.name}")
    
    def _calculate_customer_metric(self, metric_def: MetricDefinition, start_date: datetime,
                                 end_date: datetime, filters: Dict[str, Any]) -> float:
        """Calculate metrics based on customer data."""
        
        query = self.db.query(CustomerAnalytics)
        
        # Apply filters
        if 'lifecycle_stage' in filters:
            query = query.filter(CustomerAnalytics.lifecycle_stage.in_(filters['lifecycle_stage']))
        if 'min_ltv' in filters:
            query = query.filter(CustomerAnalytics.predicted_ltv >= filters['min_ltv'])
        if 'max_risk_score' in filters:
            query = query.filter(CustomerAnalytics.overall_risk_score <= filters['max_risk_score'])
        
        if metric_def.name == "total_customer_count":
            return float(query.count())
        
        elif metric_def.name == "average_customer_ltv":
            result = query.with_entities(func.avg(CustomerAnalytics.predicted_ltv)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "high_value_customer_ratio":
            total_count = query.count()
            # Top 20% by LTV
            ltv_threshold = query.with_entities(
                func.percentile_cont(0.8).within_group(CustomerAnalytics.predicted_ltv)
            ).scalar()
            high_value_count = query.filter(CustomerAnalytics.predicted_ltv >= ltv_threshold).count()
            return (high_value_count / total_count) if total_count > 0 else 0.0
        
        elif metric_def.name == "customer_churn_risk":
            result = query.with_entities(func.avg(CustomerAnalytics.churn_probability)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "active_customer_ratio":
            total_count = query.count()
            active_count = query.filter(CustomerAnalytics.lifecycle_stage == 'active').count()
            return (active_count / total_count) if total_count > 0 else 0.0
        
        elif metric_def.name == "average_account_age":
            result = query.with_entities(func.avg(CustomerAnalytics.account_age_days)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "kyc_completion_rate":
            total_count = query.count()
            completed_count = query.filter(CustomerAnalytics.kyc_status == 'completed').count()
            return (completed_count / total_count) if total_count > 0 else 0.0
        
        else:
            raise ValueError(f"Unknown customer metric: {metric_def.name}")
    
    def _calculate_performance_metric(self, metric_def: MetricDefinition, start_date: datetime,
                                    end_date: datetime, filters: Dict[str, Any]) -> float:
        """Calculate metrics based on performance data."""
        
        query = self.db.query(PerformanceMetrics).filter(
            and_(
                PerformanceMetrics.measurement_timestamp >= start_date,
                PerformanceMetrics.measurement_timestamp <= end_date
            )
        )
        
        # Apply filters
        if 'service_name' in filters:
            query = query.filter(PerformanceMetrics.service_name.in_(filters['service_name']))
        
        if metric_def.name == "average_response_time":
            result = query.with_entities(func.avg(PerformanceMetrics.response_time_ms)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "system_availability":
            result = query.with_entities(func.avg(PerformanceMetrics.transaction_success_rate)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "total_throughput":
            result = query.with_entities(func.sum(PerformanceMetrics.throughput_rps)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "average_error_rate":
            result = query.with_entities(func.avg(PerformanceMetrics.error_rate)).scalar()
            return float(result) if result else 0.0
        
        elif metric_def.name == "peak_response_time":
            result = query.with_entities(func.max(PerformanceMetrics.response_time_ms)).scalar()
            return float(result) if result else 0.0
        
        else:
            raise ValueError(f"Unknown performance metric: {metric_def.name}")
    
    def calculate_trend_analysis(self, metric_name: str, periods: int = 7,
                               period_type: str = 'daily') -> Dict[str, Any]:
        """
        Calculate trend analysis for a metric over multiple periods.
        
        Args:
            metric_name: Name of the metric to analyze
            periods: Number of periods to analyze
            period_type: Type of period ('daily', 'weekly', 'monthly')
            
        Returns:
            Dictionary containing trend analysis results
        """
        
        # Calculate period duration
        if period_type == 'daily':
            period_delta = timedelta(days=1)
        elif period_type == 'weekly':
            period_delta = timedelta(weeks=1)
        elif period_type == 'monthly':
            period_delta = timedelta(days=30)
        else:
            raise ValueError(f"Unknown period type: {period_type}")
        
        # Calculate metrics for each period
        end_date = datetime.utcnow()
        trend_data = []
        
        for i in range(periods):
            period_end = end_date - (period_delta * i)
            period_start = period_end - period_delta
            
            try:
                result = self.calculate_metric(metric_name, period_start, period_end)
                trend_data.append({
                    'period': i,
                    'start_date': period_start.isoformat(),
                    'end_date': period_end.isoformat(),
                    'value': result.value
                })
            except Exception as e:
                self.logger.error(f"Error calculating trend for period {i}: {str(e)}")
        
        # Reverse to get chronological order
        trend_data.reverse()
        
        # Calculate trend statistics
        values = [data['value'] for data in trend_data]
        
        if len(values) >= 2:
            # Linear regression for trend
            x = np.arange(len(values))
            y = np.array(values)
            slope, intercept = np.polyfit(x, y, 1)
            
            # Calculate correlation coefficient
            correlation = np.corrcoef(x, y)[0, 1] if len(values) > 1 else 0
            
            # Determine trend direction
            if slope > 0:
                trend_direction = 'increasing'
            elif slope < 0:
                trend_direction = 'decreasing'
            else:
                trend_direction = 'stable'
        else:
            slope = 0
            correlation = 0
            trend_direction = 'insufficient_data'
        
        return {
            'metric_name': metric_name,
            'period_type': period_type,
            'periods_analyzed': len(trend_data),
            'trend_data': trend_data,
            'statistics': {
                'slope': slope,
                'correlation': correlation,
                'trend_direction': trend_direction,
                'min_value': min(values) if values else 0,
                'max_value': max(values) if values else 0,
                'average_value': np.mean(values) if values else 0,
                'standard_deviation': np.std(values) if values else 0
            }
        }
    
    def calculate_comparative_analysis(self, metric_name: str, comparison_filters: List[Dict[str, Any]],
                                     start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """
        Calculate comparative analysis for a metric across different segments.
        
        Args:
            metric_name: Name of the metric to compare
            comparison_filters: List of filter sets for comparison
            start_date: Start of the analysis period
            end_date: End of the analysis period
            
        Returns:
            Dictionary containing comparative analysis results
        """
        
        comparison_results = []
        
        for i, filters in enumerate(comparison_filters):
            try:
                result = self.calculate_metric(metric_name, start_date, end_date, filters)
                comparison_results.append({
                    'segment': f"Segment_{i+1}",
                    'filters': filters,
                    'value': result.value,
                    'change_percentage': result.change_percentage
                })
            except Exception as e:
                self.logger.error(f"Error calculating comparative metric for segment {i}: {str(e)}")
        
        # Calculate comparative statistics
        values = [result['value'] for result in comparison_results]
        
        if values:
            best_segment = max(comparison_results, key=lambda x: x['value'])
            worst_segment = min(comparison_results, key=lambda x: x['value'])
            
            return {
                'metric_name': metric_name,
                'period': {
                    'start': start_date.isoformat(),
                    'end': end_date.isoformat()
                },
                'segments': comparison_results,
                'summary': {
                    'best_performing_segment': best_segment,
                    'worst_performing_segment': worst_segment,
                    'average_value': np.mean(values),
                    'value_range': max(values) - min(values),
                    'coefficient_of_variation': np.std(values) / np.mean(values) if np.mean(values) != 0 else 0
                }
            }
        else:
            return {
                'metric_name': metric_name,
                'error': 'No valid segments for comparison'
            }
    
    def calculate_percentile_analysis(self, metric_name: str, start_date: datetime,
                                    end_date: datetime, percentiles: List[float] = None) -> Dict[str, Any]:
        """
        Calculate percentile analysis for a metric.
        
        Args:
            metric_name: Name of the metric to analyze
            start_date: Start of the analysis period
            end_date: End of the analysis period
            percentiles: List of percentiles to calculate (default: [25, 50, 75, 90, 95, 99])
            
        Returns:
            Dictionary containing percentile analysis results
        """
        
        if percentiles is None:
            percentiles = [25, 50, 75, 90, 95, 99]
        
        # Get the metric definition
        if metric_name not in self._metric_definitions:
            raise ValueError(f"Unknown metric: {metric_name}")
        
        metric_def = self._metric_definitions[metric_name]
        
        # Get raw data for percentile calculation
        if metric_def.data_source == "transactions":
            query = self.db.query(TransactionAnalytics).filter(
                and_(
                    TransactionAnalytics.transaction_date >= start_date,
                    TransactionAnalytics.transaction_date <= end_date
                )
            )
            
            if metric_name == "transaction_amounts":
                values = [float(t.amount) for t in query.all()]
            elif metric_name == "risk_scores":
                values = [float(t.risk_score) for t in query.all() if t.risk_score]
            else:
                # For other metrics, calculate individual values
                values = []
                for transaction in query.all():
                    # This would need specific logic for each metric
                    pass
        
        elif metric_def.data_source == "customers":
            query = self.db.query(CustomerAnalytics)
            
            if metric_name == "customer_ltv":
                values = [float(c.predicted_ltv) for c in query.all() if c.predicted_ltv]
            elif metric_name == "customer_risk_scores":
                values = [float(c.overall_risk_score) for c in query.all() if c.overall_risk_score]
            else:
                values = []
        
        else:
            values = []
        
        if not values:
            return {
                'metric_name': metric_name,
                'error': 'No data available for percentile analysis'
            }
        
        # Calculate percentiles
        percentile_results = {}
        for p in percentiles:
            percentile_results[f"p{p}"] = float(np.percentile(values, p))
        
        return {
            'metric_name': metric_name,
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'sample_size': len(values),
            'percentiles': percentile_results,
            'statistics': {
                'min': min(values),
                'max': max(values),
                'mean': np.mean(values),
                'median': np.median(values),
                'std_dev': np.std(values)
            }
        }
    
    def _load_default_metrics(self) -> Dict[str, MetricDefinition]:
        """Load default metric definitions."""
        
        metrics = {}
        
        # Financial Metrics
        metrics["total_transaction_count"] = MetricDefinition(
            name="total_transaction_count",
            type=MetricType.FINANCIAL,
            description="Total number of transactions",
            calculation_method="count",
            aggregation=AggregationType.COUNT,
            data_source="transactions"
        )
        
        metrics["total_transaction_volume"] = MetricDefinition(
            name="total_transaction_volume",
            type=MetricType.FINANCIAL,
            description="Total transaction volume",
            calculation_method="sum",
            aggregation=AggregationType.SUM,
            data_source="transactions"
        )
        
        metrics["average_transaction_amount"] = MetricDefinition(
            name="average_transaction_amount",
            type=MetricType.FINANCIAL,
            description="Average transaction amount",
            calculation_method="average",
            aggregation=AggregationType.AVERAGE,
            data_source="transactions"
        )
        
        metrics["revenue_estimate"] = MetricDefinition(
            name="revenue_estimate",
            type=MetricType.FINANCIAL,
            description="Estimated revenue from transaction fees",
            calculation_method="custom",
            aggregation=AggregationType.SUM,
            data_source="transactions"
        )
        
        # Risk Metrics
        metrics["high_risk_transaction_ratio"] = MetricDefinition(
            name="high_risk_transaction_ratio",
            type=MetricType.RISK,
            description="Ratio of high-risk transactions",
            calculation_method="ratio",
            aggregation=AggregationType.AVERAGE,
            data_source="transactions"
        )
        
        metrics["fraud_detection_rate"] = MetricDefinition(
            name="fraud_detection_rate",
            type=MetricType.RISK,
            description="Rate of fraud detection",
            calculation_method="ratio",
            aggregation=AggregationType.AVERAGE,
            data_source="transactions"
        )
        
        metrics["average_risk_score"] = MetricDefinition(
            name="average_risk_score",
            type=MetricType.RISK,
            description="Average risk score across transactions",
            calculation_method="average",
            aggregation=AggregationType.AVERAGE,
            data_source="transactions"
        )
        
        # Customer Metrics
        metrics["total_customer_count"] = MetricDefinition(
            name="total_customer_count",
            type=MetricType.CUSTOMER,
            description="Total number of customers",
            calculation_method="count",
            aggregation=AggregationType.COUNT,
            data_source="customers"
        )
        
        metrics["average_customer_ltv"] = MetricDefinition(
            name="average_customer_ltv",
            type=MetricType.CUSTOMER,
            description="Average customer lifetime value",
            calculation_method="average",
            aggregation=AggregationType.AVERAGE,
            data_source="customers"
        )
        
        metrics["customer_churn_risk"] = MetricDefinition(
            name="customer_churn_risk",
            type=MetricType.CUSTOMER,
            description="Average customer churn risk",
            calculation_method="average",
            aggregation=AggregationType.AVERAGE,
            data_source="customers"
        )
        
        # Performance Metrics
        metrics["average_response_time"] = MetricDefinition(
            name="average_response_time",
            type=MetricType.PERFORMANCE,
            description="Average system response time",
            calculation_method="average",
            aggregation=AggregationType.AVERAGE,
            data_source="performance"
        )
        
        metrics["system_availability"] = MetricDefinition(
            name="system_availability",
            type=MetricType.PERFORMANCE,
            description="System availability percentage",
            calculation_method="average",
            aggregation=AggregationType.AVERAGE,
            data_source="performance"
        )
        
        # Compliance Metrics
        metrics["large_transaction_count"] = MetricDefinition(
            name="large_transaction_count",
            type=MetricType.COMPLIANCE,
            description="Number of large transactions requiring reporting",
            calculation_method="count",
            aggregation=AggregationType.COUNT,
            data_source="transactions",
            filters={"min_amount": 10000}
        )
        
        metrics["suspicious_activity_ratio"] = MetricDefinition(
            name="suspicious_activity_ratio",
            type=MetricType.COMPLIANCE,
            description="Ratio of suspicious activities",
            calculation_method="ratio",
            aggregation=AggregationType.AVERAGE,
            data_source="transactions"
        )
        
        return metrics
    
    def add_custom_metric(self, metric_definition: MetricDefinition):
        """Add a custom metric definition."""
        self._metric_definitions[metric_definition.name] = metric_definition
        self.logger.info(f"Added custom metric: {metric_definition.name}")
    
    def get_available_metrics(self) -> List[Dict[str, Any]]:
        """Get list of available metrics."""
        return [
            {
                'name': metric.name,
                'type': metric.type.value,
                'description': metric.description,
                'data_source': metric.data_source,
                'aggregation': metric.aggregation.value
            }
            for metric in self._metric_definitions.values()
        ]
    
    def calculate_kpi_dashboard(self, start_date: datetime, end_date: datetime) -> Dict[str, Any]:
        """Calculate a comprehensive KPI dashboard."""
        
        key_metrics = [
            "total_transaction_count",
            "total_transaction_volume",
            "average_transaction_amount",
            "revenue_estimate",
            "high_risk_transaction_ratio",
            "fraud_detection_rate",
            "total_customer_count",
            "average_customer_ltv",
            "average_response_time",
            "system_availability"
        ]
        
        results = self.calculate_multiple_metrics(key_metrics, start_date, end_date)
        
        # Format for dashboard
        kpi_data = {}
        for result in results:
            kpi_data[result.metric_name] = {
                'value': result.value,
                'previous_value': result.previous_value,
                'change_percentage': result.change_percentage,
                'trend': 'up' if result.change_percentage and result.change_percentage > 0 else 'down'
            }
        
        return {
            'period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'kpis': kpi_data,
            'generated_at': datetime.utcnow().isoformat()
        }

